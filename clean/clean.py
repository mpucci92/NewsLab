from elasticsearch.helpers.errors import BulkIndexError
from elasticsearch import Elasticsearch, helpers
from const import DIR, CONFIG, logger
from clean_item import clean_item
from importlib import reload
from hashlib import sha256
from pathlib import Path
import requests
import shutil
import sys, os
import json
import time
import uuid

sys.path.append(f"{DIR}/..")
from utils import send_metric

###################################################################################################

# ES_CLIENT = Elasticsearch(CONFIG['ES']['IP'], port=CONFIG['ES']['PORT'], http_comprress=True, timeout=30)
ES_CLIENT = Elasticsearch()
HEADERS = {"Content-Type" : "application/json"}

NEWS_DIRS = [
	Path(f"{DIR}/../rss/news_data"),
	Path(f"{DIR}/../news/news_data"),
]

NEWS_DIR = Path(f"{DIR}/news_data")
CLEAN_DIR = Path(f"{DIR}/clean_data")

###################################################################################################

def get_files(files):
	
	return [
		shutil.copy(file, NEWS_DIR / file.name)
		for _dir in NEWS_DIRS
		for file in list(_dir.iterdir())
		if
		(
			(NEWS_DIR / file.name) not in files
			and
			file.name != '.gitignore'
		) 
	]

def get_scores(sentences):

	data = {"sentences" : sentences}
	response = requests.post("http://localhost:9602", headers=HEADERS, json=data)
	response = json.loads(response.content)
	return response.values()

def cleaning_loop():

	files = {NEWS_DIR / ".gitignore"}
	n_clean = len(list(CLEAN_DIR.iterdir()))

	while True:

		new_files = get_files(files)
		n_clean_new = len(list(CLEAN_DIR.iterdir()))

		if n_clean_new < n_clean:
			files = {NEWS_DIR / ".gitignore"}
			reload(sys.modules['find_company_names'])
			logger.info("reloading the company names")

		n_clean = n_clean_new
				
		items = []
		for new_file in new_files:
			with open(new_file, "r") as file:
				try:
					items.extend(json.loads(file.read()))
					files.add(new_file)
				except Exception as e:
					logger.warning(f"File read error. {e}")

		new_items = []
		for item in items:

			if not item.get("title"):
				continue

			item = clean_item(item)

			dummy_item = {
				"title" : item['title'].lower(),
				"link" : item['link'].lower()
			}
			
			summary = item.get('summary')
			if summary:
				dummy_item['summary'] = summary

			dummy_item = json.dumps(dummy_item, sort_keys = True)
			_hash = sha256(dummy_item.encode()).hexdigest()

			new_items.append({
				"_index" : "news",
				"_id" : _hash,
				"_op_type" : "create",
				"_source" : item
			})

		if len(new_items) != 0:

			titles = [
				item['_source']['title']
				for item in new_items
			]
			scores = get_scores(titles)

			for item, score in zip(new_items, scores):
				item['_source']['sentiment'] = score['prediction']
				item['_source']['sentiment_score'] = score['sentiment_score']
				item['_source']['abs_sentiment_score'] = abs(score['sentiment_score'])	

			successes, failures = helpers.bulk(ES_CLIENT,
											   new_items,
											   stats_only=True,
											   raise_on_error=False)
			
			print(successes, failures)
			with open(CLEAN_DIR / f"{str(uuid.uuid4())}.json", "w") as file:
				file.write(json.dumps(new_items))

			new_items = []

		###########################################################################################

		try:
			
			send_metric(
				CONFIG,
				"rss_counter",
				"int64_value",
				len(list(NEWS_DIRS[0].iterdir())) - 1
			)

		except Exception as e:

			logger.warning(e)

		time.sleep(3)

if __name__ == '__main__':

	cleaning_loop()
